{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[{"file_id":"https://github.com/woctezuma/stable-diffusion-colab/blob/main/stable_diffusion.ipynb","timestamp":1706004446107}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["%pip install --quiet --upgrade diffusers transformers accelerate mediapy"],"metadata":{"id":"ufD_d64nr08H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import mediapy as media\n","import random\n","import sys\n","import torch\n","\n","from diffusers import AutoPipelineForText2Image\n","\n","pipe = AutoPipelineForText2Image.from_pretrained(\n","    \"stabilityai/sdxl-turbo\",\n","    torch_dtype=torch.float16,\n","    use_safetensors=True,\n","    variant=\"fp16\",\n","    )\n","\n","pipe = pipe.to(\"cuda\")"],"metadata":{"id":"bG2hkmSEvByV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def remove_blank_lines(input_file, output_file):\n","    with open(input_file, 'r') as infile, open(output_file, 'w') as outfile:\n","        for line in infile:\n","            if line.strip():  # Check if the line is not just whitespace\n","                outfile.write(line)\n","\n","# Example usage:\n","input_file_path = '/content/school ahead.txt'\n","output_file_path = '/content/pedestrain1 f.txt'\n","\n","remove_blank_lines(input_file_path, output_file_path)"],"metadata":{"id":"E8PPmTRFjTaL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os"],"metadata":{"id":"L6IYVsPeT4p0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Assuming you have a function 'save_image' to save the generated image\n","def save_image(image, output_path):\n","    image.save(output_path)\n","\n","input_file = \"/content/final prompts.txt\"  # Replace with the path to your input file\n","output_folder = \"/content/finalimages\"   # Replace with the desired output folder\n","\n","# Create the output folder if it doesn't exist\n","os.makedirs(output_folder, exist_ok=True)\n","\n","num_inference_steps = 4\n","seed = random.randint(0, sys.maxsize)\n","\n","# Read prompts from the input file\n","with open(input_file, 'r') as file:\n","    prompts = file.readlines()\n","\n","# Iterate through each prompt\n","for i, prompt in enumerate(prompts):\n","    prompt = prompt.strip()\n","\n","    # Generate images for the current prompt\n","    images = pipe(\n","        prompt=prompt,\n","        guidance_scale=0.8,  # Adjust the guidance scale as needed\n","        viewpoint_guidance=\"outside\",  # Add a parameter for viewpoint guidance\n","        num_inference_steps=num_inference_steps,\n","        generator=torch.Generator(\"cuda\").manual_seed(seed),\n","    ).images\n","\n","    # Save the generated images in the output folder\n","    for j, image in enumerate(images):\n","        output_path = os.path.join(output_folder, f\"{i + 1}.jpg\")\n","        save_image(image, output_path)\n","\n","    print(f\"Generated images for prompt {i + 1}\")\n","\n","print(\"All images generated and saved.\")\n"],"metadata":{"id":"AUc4QJfE-uR9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"LclHQy2myGhg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%cd /content/sample_data/140images\n"],"metadata":{"id":"nVESxMGO0Xrj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!zip -r newped1 /content/pedestrain_new1"],"metadata":{"id":"76uuAR770lwT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import files\n","files.download('/content/schoolahead1.zip')"],"metadata":{"id":"DnCHoslZtaSV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import re\n","\n","def replace_t_with_y(text):\n","    return re.sub(r'T', 'Y', text)\n","\n","def main():\n","    # Input file path\n","    file_path = input(\"/content/T junction new.txt\")\n","\n","    try:\n","        # Read the content of the file\n","        with open(file_path, 'r') as file:\n","            original_text = file.read()\n","\n","        # Replace 'T' with 'Y'\n","        modified_text = replace_t_with_y(original_text)\n","\n","        # Output the modified text\n","        print(\"Original text:\")\n","        print(original_text)\n","        print(\"\\nModified text:\")\n","        print(modified_text)\n","\n","    except FileNotFoundError:\n","        print(\"File not found. Please check the file path.\")\n","\n","if __name__ == \"__main__\":\n","    main()\n"],"metadata":{"id":"gAWB7YD5y6ua"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from diffusers.utils import load_image\n","from diffusers import StableDiffusionXLControlNetInpaintPipeline, ControlNetModel\n","from transformers import pipeline\n","from PIL import Image\n","import numpy as np\n","\n","# Configuration\n","IS_UNDER_EXPOSURE = False #change this option for output underexposured ball\n","if IS_UNDER_EXPOSURE:\n","    PROMPT = \"a perfect Red car going through the hills with clear sky\"\n","else:\n","    PROMPT = \"a perfect Red car going through the hills with clear sky\"\n","\n","NEGATIVE_PROMPT = \"matte, diffuse, flat, dull\"\n","IMAGE_URL = \"/content/download.jpeg\"\n","\n","# load pipeline\n","controlnet = ControlNetModel.from_pretrained(\"diffusers/controlnet-depth-sdxl-1.0\", torch_dtype=torch.float16)\n","pipe = StableDiffusionXLControlNetInpaintPipeline.from_pretrained(\n","    \"stabilityai/stable-diffusion-xl-base-1.0\",\n","    controlnet=controlnet,\n","    torch_dtype=torch.float16,\n",").to(\"cuda\")\n","pipe.load_lora_weights(\"DiffusionLight/DiffusionLight\")\n","pipe.fuse_lora(lora_scale=0.75)\n","depth_estimator = pipeline(task=\"depth-estimation\", model=\"Intel/dpt-large\")\n","\n","# prepare input image\n","init_image = load_image(IMAGE_URL)\n","depth_image = depth_estimator(images=init_image)['depth']\n","\n","# create mask and depth map with mask for inpainting\n","def get_circle_mask(size=256):\n","    x = torch.linspace(-1, 1, size)\n","    y = torch.linspace(1, -1, size)\n","    y, x = torch.meshgrid(y, x)\n","    z = (1 - x**2 - y**2)\n","    mask = z >= 0\n","    return mask\n","mask = get_circle_mask().numpy()\n","depth = np.asarray(depth_image).copy()\n","depth[384:640, 384:640] = depth[384:640, 384:640] * (1 - mask) + (mask * 255)\n","depth_mask = Image.fromarray(depth)\n","mask_image = np.zeros_like(depth)\n","mask_image[384:640, 384:640] = mask * 255\n","mask_image = Image.fromarray(mask_image)\n","\n","# run the pipeline\n","output = pipe(\n","    prompt=PROMPT,\n","    negative_prompt=NEGATIVE_PROMPT,\n","    num_inference_steps=30,\n","    image=init_image,\n","    mask_image=mask_image,\n","    control_image=depth_mask,\n","    controlnet_conditioning_scale=0.5,\n",")\n","\n","# save output\n","output[\"images\"][0].save(\"output.png\")\n"],"metadata":{"id":"2HOhoFyRtT4n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install diffusers"],"metadata":{"id":"ZYnNYr01uF4E"},"execution_count":null,"outputs":[]}]}